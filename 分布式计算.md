分布式计算演示
----------

## 直接执行MR任务

直接运行：`hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount /input /output`

用`jps`观察进程，发现多了一个`RunJar`之外，进程没有变化。

查看日志：
```
2020-07-28 07:46:42,729 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2020-07-28 07:46:42,777 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2020-07-28 07:46:42,777 INFO impl.MetricsSystemImpl: JobTracker metrics system started
2020-07-28 07:46:42,984 INFO input.FileInputFormat: Total input files to process : 9
2020-07-28 07:46:43,002 INFO mapreduce.JobSubmitter: number of splits:9
2020-07-28 07:46:43,090 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1033767748_0001
2020-07-28 07:46:43,090 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-07-28 07:46:43,168 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
2020-07-28 07:46:43,169 INFO mapreduce.Job: Running job: job_local1033767748_0001
2020-07-28 07:46:43,169 INFO mapred.LocalJobRunner: OutputCommitter set in config null
...
2020-07-28 07:46:44,174 INFO mapreduce.Job:  map 100% reduce 100%
2020-07-28 07:46:44,175 INFO mapreduce.Job: Job job_local1033767748_0001 completed successfully
...
```


直接运行MR任务，将以Local模式运行

## 配置Yarn运行

1. 修改配置：

增加配置`mapred-site.xml`：

```xml
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
    <property>
      <name>yarn.app.mapreduce.am.env</name>
      <value>HADOOP_MAPRED_HOME=/app/hadoop/</value>
    </property>
    <property>
      <name>mapreduce.map.env</name>
      <value>HADOOP_MAPRED_HOME=/app/hadoop/</value>
    </property>
    <property>
      <name>mapreduce.reduce.env</name>
      <value>HADOOP_MAPRED_HOME=/app/hadoop/</value>
    </property>
```

增加配置`yarn-site.xml`:

```xml
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hd01-1</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <!-- 启动一个额外的组件，以处理MR任务 -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
```

增加配置`hadoop-env.sh`:

```bash
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```

2. 启动yarn服务: `start-yarn.sh`. 

可以观察到每个节点都多了两个进程`ResourceManager`与`NodeManager`。

使用`yarn node -list`查看可用的计算node，可发现我们配置的两个节点。

使用`yarn application -list -appStates ALL`查看运行的分布式应用，当前没有应用。

3. 运行MR任务: 

```bash
hdfs dfs -rm -r /output
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount /input /output
```

此时运行`yarn application -list -appStates ALL`将能看到运行的应用。

找到应用的id，然后运行`yarn logs -applicationId ${APP_ID}`可以查看应用运行日志。


## 任务调度

### 默认FIFO调度

在两个窗口中分别执行`hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount /input /output`和`hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount /input /output1`

在一个新窗口执行`watch yarn application -list`可以观察到后提交的任务一直是`ACCEPTTED`状态，直到第一个任务运行完毕，才转为`RUNNING`执行。

### 配置Capacity调度

1. 修改配置

修改`yarn-site.xml`，增加配置：

```xml
    <property>
        <name>yarn.resourcemanager.scheduler.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
    </property>
```

修改`capacity-scheduler.xml`，修改配置：

```xml
  <property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>default,a,b</value>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.capacity</name>
    <value>30</value>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.a.capacity</name>
    <value>35</value>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.b.capacity</name>
    <value>35</value>
  </property>
```

2. 重启yarn: `stop-yarn.sh && start-yarn.sh`

3. 测试Capacity调度器

在两个窗口分别运行：
`hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount -D mapreduce.job.queuename=a /input /output1`
`hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount -D mapreduce.job.queuename=b /input /output1`

在另一个窗口运行`watch yarn application -list`。可以观察到两个任务都处于`RUNNING`状态，并行执行。

Capacity调度器还支持其他的策略，比如配置最大、最小可用容量，配置memory/vcore数量，支持多租户，支持自动按用户和组来创建队列，根据规则选择队列，支持任务优先级，抢占等。大家可以进一步做尝试。


### 配置公平调度

公平调度是最复杂的调度器了，因为公平本身很难定义和实现。它可以支持多级队列，可以基于内存或cpu请求量进行公平决策。

1. 修改配置

修改`yarn-site.xml`:

```xml
<property>
  <name>yarn.resourcemanager.scheduler.class</name>
  <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
</property>
```

在两个窗口分别运行：
`hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount -D mapreduce.job.queuename=a /input /output1`
`hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount -D mapreduce.job.queuename=b /input /output1`

在另一个窗口运行`watch yarn application -list`。可以观察到两个任务都处于`RUNNING`状态，并行执行。

Fair调度器支持其他复杂配置，比如配置最大、最小可用容量，权重，抢占，保留，自定义策略等。大家可以进一步做尝试。




